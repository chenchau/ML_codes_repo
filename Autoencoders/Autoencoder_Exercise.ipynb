{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Autoencoder_Exercise.ipynb","provenance":[],"authorship_tag":"ABX9TyNx0WpciG0mJ1KctRm4UPNo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zCPrWr7uTPFl","colab_type":"text"},"source":["# Fully-connected autoencoders"]},{"cell_type":"markdown","metadata":{"id":"NWKGaYyapdG1","colab_type":"text"},"source":["**Train an autoencoder on MNIST dataset**\n","\n","tasks (all using fc layer in encoder / decoder part):\n","\n","\n","*   linear autoencoder\n","    *   MSE results on test data\n","    *   visualizatoin of weights (first fc layer) \n","\n","*   non-linear autoencoder\n","   *   MSE results on test data\n","   *   visualization of weights (first fc layer)\n","\n","*   sparse autoencoder\n","    *   MSE results on test data\n","    *   visualization of weights (first fc layer)\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"MKflJE57p4pr","colab_type":"code","colab":{}},"source":["####### importing python packages ######\n","import os\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from torchvision.utils import save_image \n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# make dir to save images\n","if not os.path.exists('./in_out_img'):\n","    os.mkdir('./in_out_img')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoGunxGoqU3-","colab_type":"text"},"source":["**training configs (user defined)**      \n","**please adjust the training hyperparameters and architectures in this part**"]},{"cell_type":"code","metadata":{"id":"SaCCOby6qTbp","colab_type":"code","colab":{}},"source":["###### training configs -- user defined ######\n","num_epochs = 100\n","batch_size = 128\n","learning_rate = 1e-3\n","momentum = 0.9          # momentum\n","weight_decay =1e-5      # weight decay\n","\n","arch = 'linear_AE'   # 'nonlinear_AE' | 'linear_AE' | 'sparse_AE' \n","sparse_reg  =  1e-3   #  regulaization coefficient\n","best_val = float('inf')     # record the best val loss\n","# test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1_M5rFSIwHW","colab_type":"text"},"source":["**models (nonlinear_AE  |  linear_AE  |  sparse_AE)** "]},{"cell_type":"code","metadata":{"id":"GUq0sMF-J2hO","colab_type":"code","colab":{}},"source":["# define the non-linear autoencoder  128 -- 64 -- 16 -- 64 -- 128\n","class nonlinear_AE(nn.Module):\n","    def __init__(self):\n","        super(nonlinear_AE, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(28 * 28, 128),\n","            nn.ReLU(True),\n","            nn.Linear(128, 64),\n","            nn.ReLU(True), \n","            nn.Linear(64, 32), \n","            nn.ReLU(True))\n","        self.decoder = nn.Sequential(\n","            nn.Linear(32, 64),\n","            nn.ReLU(True),\n","            nn.Linear(64, 128),\n","            nn.ReLU(True), \n","            nn.Linear(128, 28 * 28), \n","            nn.Tanh())\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","\n","# define the linear autoencoder  128 -- 64 -- 16 -- 64 -- 128\n","class linear_AE(nn.Module):\n","    def __init__(self):\n","        super(linear_AE, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(28 * 28, 128),\n","            # nn.ReLU(True),\n","            nn.Linear(128, 64),\n","            # nn.ReLU(True), \n","            nn.Linear(64, 32), \n","            # nn.ReLU(True)\n","            )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(32, 64),\n","            # nn.ReLU(True),\n","            nn.Linear(64, 128),\n","            # nn.ReLU(True), \n","            nn.Linear(128, 28 * 28), \n","            # nn.Tanh()\n","            )\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","\n","class sparse_AE(nn.Module):\n","    def __init__(self):\n","        super(sparse_AE, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(28*28, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(128, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, 32),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            nn.Linear(32, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(128, 28*28),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# define the sparse loss, here using mean absolute error (L1 loss)\n","def sparse_loss(autoencoder, inputs):\n","    loss = 0\n","    out = inputs\n","    for i in range(3):  # encoder layers constraining on the activations\n","        fc_layer = list(autoencoder.encoder.children())[2 * i]  # 0, 2, 4\n","        relu = list(autoencoder.encoder.children())[2 * i + 1]\n","        out = relu(fc_layer(out))\n","        loss += torch.mean(torch.abs(out))\n","    for i in range(2):  # decoder layers (without the last fc layer) constraining on the activations\n","        fc_layer = list(autoencoder.decoder.children())[2 * i]\n","        relu = list(autoencoder.decoder.children())[2 * i + 1]\n","        out = relu(fc_layer(out))\n","        loss += torch.mean(torch.abs(out))\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7XRcP8Mj8TW","colab_type":"text"},"source":["**define the evaluation**"]},{"cell_type":"code","metadata":{"id":"k87tzdCDWqdj","colab_type":"code","colab":{}},"source":["def evaluation(model, test_loader, epoch):\n","    total_loss = 0\n","    recons_loss = nn.MSELoss()\n","    model.eval()\n","    for batch_idx, (img, _) in enumerate(test_loader):\n","        img = img.view(img.size(0), -1)\n","        img = Variable(img.cuda())\n","        out = model(img)\n","        loss = recons_loss(out, img)\n","        total_loss += loss * img.size(0)\n","    avg_loss = total_loss / len(test_loader.dataset)\n","\n","    print('\\nAverage MSE Loss on Test set: {:.4f}'.format(avg_loss))\n","    if epoch % 10 == 0:\n","        # save input/output images\n","        pic = to_img(img.cpu().data)\n","        save_image(pic, './in_out_img/input_{}.png'.format(epoch))\n","        pic = to_img(out.cpu().data)\n","        save_image(pic, './in_out_img/recons_{}.png'.format(epoch))\n","    # save the best model with lowest mse loss    \n","    global best_val\n","    if avg_loss < best_val:\n","        best_val = avg_loss\n","        torch.save(model.state_dict(), './' + arch + '_best.pth')\n","    print('Saved Best Model\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0xK28PFveAlj","colab_type":"text"},"source":["**start training (use Adam optimizer)**"]},{"cell_type":"code","metadata":{"id":"WDd5u5jVmN-r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1569693888101,"user_tz":240,"elapsed":1464032,"user":{"displayName":"Chen Zhou","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAPQT2RBEruCVhGe2sNzB-3zrFI3wfjiLepdszV=s64","userId":"08552159914179628257"}},"outputId":"1dcc82ca-b423-4e3c-b6cb-413dbdfc3a27"},"source":["# coder credits: https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/simple_autoencoder.py\n","# fix random seed for reproducible results\n","seed = 0\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","\n","# define method to save output images \n","def to_img(x):\n","    x = 0.5 * (x + 1)   # Tanh activation squashes the output to range [-1, 1] \n","    x = x.clamp(0, 1)   \n","    x = x.view(x.size(0), 1, 28, 28)    # reshae the vectorized output to grayscale images\n","    return x\n","\n","# define the transformation applied to MNIST images\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# train/test loader\n","train_loader = DataLoader(MNIST('./data', train=True, download=True, \n","                          transform=transform), batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(MNIST('./data', train=False, \n","                          transform=transform), batch_size=batch_size, shuffle=False)\n","\n","\n","if arch == 'nonlinear_AE':  \n","    model = nonlinear_AE().cuda()\n","    # model.load_state_dict(torch.load('./.pth'))\n","elif arch == 'linear_AE':\n","    model = linear_AE().cuda()\n","elif arch == 'sparse_AE':\n","    model = sparse_AE().cuda()\n","\n","print('training {} model'.format(arch))\n","\n","# use Mean Squared Error loss for reconstruction\n","recons_loss = nn.MSELoss()\n","\n","# define the optimizer\n","# optimizer =  torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","print('Adam optimizer')\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","for epoch in range(num_epochs):\n","    model.train()     # train mode\n","    #  no use of labels\n","    for batch_idx, (img, _) in enumerate(train_loader):\n","        # show the first image in the 'batch_id'-th batch\n","        # plt.imshow((data[0].numpy().copy().squeeze(0)), cmap='gray')\n","        # strech images into column vectors\n","        img = img.view(img.size(0), -1)\n","        img = Variable(img).cuda()\n","        # ===================forward=====================\n","        # output reconstructed images\n","        out = model(img)\n","        \n","        if arch == 'sparse_AE':  # if sparse autoencoder, use MSE loss and L1 loss\n","            l1_loss = sparse_loss(model, img)\n","            mse_loss = recons_loss(out, img)\n","            loss = mse_loss + sparse_reg * l1_loss\n","        else: # compute the MSE loss for models w/o regularization\n","            loss = recons_loss(out, img)\n","        # ===================backward====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # ===================log========================\n","        if (batch_idx + 1) % 100 == 0:\n","            if arch == 'sparse_AE':\n","                print('Epoch [{}/{}] - Iter[{}/{}], Total loss:{:.4f}, MSE loss:{:.4f}, Sparse loss:{:.4f}'.format(\n","                    epoch + 1, num_epochs, batch_idx + 1, len(train_loader.dataset) // batch_size, loss.item(), mse_loss.item(), l1_loss.item()))\n","            else:\n","                print('Epoch [{}/{}] - Iter[{}/{}], MSE loss:{:.4f}'.format(\n","                    epoch + 1, num_epochs, batch_idx + 1, len(train_loader.dataset) // batch_size, loss.item()))\n","    # evaluation on test set\n","    evaluation(model, test_loader, epoch)\n","    # ===================log========================\n","    # print('epoch [{}/{}], MSE loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n","    \n","# save the model to file\n","torch.save(model.state_dict(), './' + arch + '_final.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["9920512it [00:01, 8839318.71it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/28881 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 135510.24it/s]           \n","  0%|          | 0/1648877 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["1654784it [00:00, 2476199.08it/s]                           \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 51793.47it/s]            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n","training linear_AE model\n","Adam optimizer\n","Epoch [1/100] - Iter[100/468], MSE loss:0.1579\n","Epoch [1/100] - Iter[200/468], MSE loss:0.1153\n","Epoch [1/100] - Iter[300/468], MSE loss:0.0986\n","Epoch [1/100] - Iter[400/468], MSE loss:0.0897\n","\n","Average MSE Loss on Test set: 0.0834\n","Saved Best Model\n","\n","Epoch [2/100] - Iter[100/468], MSE loss:0.0762\n","Epoch [2/100] - Iter[200/468], MSE loss:0.0756\n","Epoch [2/100] - Iter[300/468], MSE loss:0.0743\n","Epoch [2/100] - Iter[400/468], MSE loss:0.0716\n","\n","Average MSE Loss on Test set: 0.0699\n","Saved Best Model\n","\n","Epoch [3/100] - Iter[100/468], MSE loss:0.0749\n","Epoch [3/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [3/100] - Iter[300/468], MSE loss:0.0705\n","Epoch [3/100] - Iter[400/468], MSE loss:0.0726\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [4/100] - Iter[100/468], MSE loss:0.0675\n","Epoch [4/100] - Iter[200/468], MSE loss:0.0720\n","Epoch [4/100] - Iter[300/468], MSE loss:0.0698\n","Epoch [4/100] - Iter[400/468], MSE loss:0.0744\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [5/100] - Iter[100/468], MSE loss:0.0708\n","Epoch [5/100] - Iter[200/468], MSE loss:0.0677\n","Epoch [5/100] - Iter[300/468], MSE loss:0.0670\n","Epoch [5/100] - Iter[400/468], MSE loss:0.0729\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [6/100] - Iter[100/468], MSE loss:0.0721\n","Epoch [6/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [6/100] - Iter[300/468], MSE loss:0.0716\n","Epoch [6/100] - Iter[400/468], MSE loss:0.0704\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [7/100] - Iter[100/468], MSE loss:0.0701\n","Epoch [7/100] - Iter[200/468], MSE loss:0.0701\n","Epoch [7/100] - Iter[300/468], MSE loss:0.0701\n","Epoch [7/100] - Iter[400/468], MSE loss:0.0731\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [8/100] - Iter[100/468], MSE loss:0.0654\n","Epoch [8/100] - Iter[200/468], MSE loss:0.0730\n","Epoch [8/100] - Iter[300/468], MSE loss:0.0675\n","Epoch [8/100] - Iter[400/468], MSE loss:0.0732\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [9/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [9/100] - Iter[200/468], MSE loss:0.0758\n","Epoch [9/100] - Iter[300/468], MSE loss:0.0668\n","Epoch [9/100] - Iter[400/468], MSE loss:0.0702\n","\n","Average MSE Loss on Test set: 0.0686\n","Saved Best Model\n","\n","Epoch [10/100] - Iter[100/468], MSE loss:0.0701\n","Epoch [10/100] - Iter[200/468], MSE loss:0.0699\n","Epoch [10/100] - Iter[300/468], MSE loss:0.0670\n","Epoch [10/100] - Iter[400/468], MSE loss:0.0704\n","\n","Average MSE Loss on Test set: 0.0682\n","Saved Best Model\n","\n","Epoch [11/100] - Iter[100/468], MSE loss:0.0716\n","Epoch [11/100] - Iter[200/468], MSE loss:0.0715\n","Epoch [11/100] - Iter[300/468], MSE loss:0.0663\n","Epoch [11/100] - Iter[400/468], MSE loss:0.0700\n","\n","Average MSE Loss on Test set: 0.0682\n","Saved Best Model\n","\n","Epoch [12/100] - Iter[100/468], MSE loss:0.0712\n","Epoch [12/100] - Iter[200/468], MSE loss:0.0689\n","Epoch [12/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [12/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [13/100] - Iter[100/468], MSE loss:0.0737\n","Epoch [13/100] - Iter[200/468], MSE loss:0.0686\n","Epoch [13/100] - Iter[300/468], MSE loss:0.0691\n","Epoch [13/100] - Iter[400/468], MSE loss:0.0653\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [14/100] - Iter[100/468], MSE loss:0.0726\n","Epoch [14/100] - Iter[200/468], MSE loss:0.0714\n","Epoch [14/100] - Iter[300/468], MSE loss:0.0713\n","Epoch [14/100] - Iter[400/468], MSE loss:0.0719\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [15/100] - Iter[100/468], MSE loss:0.0697\n","Epoch [15/100] - Iter[200/468], MSE loss:0.0694\n","Epoch [15/100] - Iter[300/468], MSE loss:0.0705\n","Epoch [15/100] - Iter[400/468], MSE loss:0.0700\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [16/100] - Iter[100/468], MSE loss:0.0695\n","Epoch [16/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [16/100] - Iter[300/468], MSE loss:0.0720\n","Epoch [16/100] - Iter[400/468], MSE loss:0.0727\n","\n","Average MSE Loss on Test set: 0.0682\n","Saved Best Model\n","\n","Epoch [17/100] - Iter[100/468], MSE loss:0.0702\n","Epoch [17/100] - Iter[200/468], MSE loss:0.0709\n","Epoch [17/100] - Iter[300/468], MSE loss:0.0661\n","Epoch [17/100] - Iter[400/468], MSE loss:0.0681\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [18/100] - Iter[100/468], MSE loss:0.0682\n","Epoch [18/100] - Iter[200/468], MSE loss:0.0696\n","Epoch [18/100] - Iter[300/468], MSE loss:0.0666\n","Epoch [18/100] - Iter[400/468], MSE loss:0.0680\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [19/100] - Iter[100/468], MSE loss:0.0677\n","Epoch [19/100] - Iter[200/468], MSE loss:0.0676\n","Epoch [19/100] - Iter[300/468], MSE loss:0.0699\n","Epoch [19/100] - Iter[400/468], MSE loss:0.0691\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [20/100] - Iter[100/468], MSE loss:0.0682\n","Epoch [20/100] - Iter[200/468], MSE loss:0.0651\n","Epoch [20/100] - Iter[300/468], MSE loss:0.0702\n","Epoch [20/100] - Iter[400/468], MSE loss:0.0703\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [21/100] - Iter[100/468], MSE loss:0.0736\n","Epoch [21/100] - Iter[200/468], MSE loss:0.0714\n","Epoch [21/100] - Iter[300/468], MSE loss:0.0690\n","Epoch [21/100] - Iter[400/468], MSE loss:0.0687\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [22/100] - Iter[100/468], MSE loss:0.0691\n","Epoch [22/100] - Iter[200/468], MSE loss:0.0668\n","Epoch [22/100] - Iter[300/468], MSE loss:0.0667\n","Epoch [22/100] - Iter[400/468], MSE loss:0.0730\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [23/100] - Iter[100/468], MSE loss:0.0707\n","Epoch [23/100] - Iter[200/468], MSE loss:0.0680\n","Epoch [23/100] - Iter[300/468], MSE loss:0.0700\n","Epoch [23/100] - Iter[400/468], MSE loss:0.0699\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [24/100] - Iter[100/468], MSE loss:0.0666\n","Epoch [24/100] - Iter[200/468], MSE loss:0.0705\n","Epoch [24/100] - Iter[300/468], MSE loss:0.0697\n","Epoch [24/100] - Iter[400/468], MSE loss:0.0694\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [25/100] - Iter[100/468], MSE loss:0.0710\n","Epoch [25/100] - Iter[200/468], MSE loss:0.0699\n","Epoch [25/100] - Iter[300/468], MSE loss:0.0753\n","Epoch [25/100] - Iter[400/468], MSE loss:0.0711\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [26/100] - Iter[100/468], MSE loss:0.0723\n","Epoch [26/100] - Iter[200/468], MSE loss:0.0703\n","Epoch [26/100] - Iter[300/468], MSE loss:0.0641\n","Epoch [26/100] - Iter[400/468], MSE loss:0.0702\n","\n","Average MSE Loss on Test set: 0.0681\n","Saved Best Model\n","\n","Epoch [27/100] - Iter[100/468], MSE loss:0.0667\n","Epoch [27/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [27/100] - Iter[300/468], MSE loss:0.0700\n","Epoch [27/100] - Iter[400/468], MSE loss:0.0726\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [28/100] - Iter[100/468], MSE loss:0.0688\n","Epoch [28/100] - Iter[200/468], MSE loss:0.0689\n","Epoch [28/100] - Iter[300/468], MSE loss:0.0712\n","Epoch [28/100] - Iter[400/468], MSE loss:0.0690\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [29/100] - Iter[100/468], MSE loss:0.0686\n","Epoch [29/100] - Iter[200/468], MSE loss:0.0706\n","Epoch [29/100] - Iter[300/468], MSE loss:0.0718\n","Epoch [29/100] - Iter[400/468], MSE loss:0.0689\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [30/100] - Iter[100/468], MSE loss:0.0698\n","Epoch [30/100] - Iter[200/468], MSE loss:0.0696\n","Epoch [30/100] - Iter[300/468], MSE loss:0.0674\n","Epoch [30/100] - Iter[400/468], MSE loss:0.0653\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [31/100] - Iter[100/468], MSE loss:0.0670\n","Epoch [31/100] - Iter[200/468], MSE loss:0.0718\n","Epoch [31/100] - Iter[300/468], MSE loss:0.0682\n","Epoch [31/100] - Iter[400/468], MSE loss:0.0706\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [32/100] - Iter[100/468], MSE loss:0.0692\n","Epoch [32/100] - Iter[200/468], MSE loss:0.0730\n","Epoch [32/100] - Iter[300/468], MSE loss:0.0709\n","Epoch [32/100] - Iter[400/468], MSE loss:0.0711\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [33/100] - Iter[100/468], MSE loss:0.0673\n","Epoch [33/100] - Iter[200/468], MSE loss:0.0687\n","Epoch [33/100] - Iter[300/468], MSE loss:0.0676\n","Epoch [33/100] - Iter[400/468], MSE loss:0.0694\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [34/100] - Iter[100/468], MSE loss:0.0663\n","Epoch [34/100] - Iter[200/468], MSE loss:0.0736\n","Epoch [34/100] - Iter[300/468], MSE loss:0.0699\n","Epoch [34/100] - Iter[400/468], MSE loss:0.0737\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [35/100] - Iter[100/468], MSE loss:0.0659\n","Epoch [35/100] - Iter[200/468], MSE loss:0.0694\n","Epoch [35/100] - Iter[300/468], MSE loss:0.0697\n","Epoch [35/100] - Iter[400/468], MSE loss:0.0713\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [36/100] - Iter[100/468], MSE loss:0.0711\n","Epoch [36/100] - Iter[200/468], MSE loss:0.0693\n","Epoch [36/100] - Iter[300/468], MSE loss:0.0729\n","Epoch [36/100] - Iter[400/468], MSE loss:0.0678\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [37/100] - Iter[100/468], MSE loss:0.0679\n","Epoch [37/100] - Iter[200/468], MSE loss:0.0648\n","Epoch [37/100] - Iter[300/468], MSE loss:0.0697\n","Epoch [37/100] - Iter[400/468], MSE loss:0.0712\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [38/100] - Iter[100/468], MSE loss:0.0700\n","Epoch [38/100] - Iter[200/468], MSE loss:0.0679\n","Epoch [38/100] - Iter[300/468], MSE loss:0.0722\n","Epoch [38/100] - Iter[400/468], MSE loss:0.0678\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [39/100] - Iter[100/468], MSE loss:0.0673\n","Epoch [39/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [39/100] - Iter[300/468], MSE loss:0.0729\n","Epoch [39/100] - Iter[400/468], MSE loss:0.0711\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [40/100] - Iter[100/468], MSE loss:0.0706\n","Epoch [40/100] - Iter[200/468], MSE loss:0.0751\n","Epoch [40/100] - Iter[300/468], MSE loss:0.0747\n","Epoch [40/100] - Iter[400/468], MSE loss:0.0702\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [41/100] - Iter[100/468], MSE loss:0.0713\n","Epoch [41/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [41/100] - Iter[300/468], MSE loss:0.0699\n","Epoch [41/100] - Iter[400/468], MSE loss:0.0699\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [42/100] - Iter[100/468], MSE loss:0.0723\n","Epoch [42/100] - Iter[200/468], MSE loss:0.0699\n","Epoch [42/100] - Iter[300/468], MSE loss:0.0696\n","Epoch [42/100] - Iter[400/468], MSE loss:0.0710\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [43/100] - Iter[100/468], MSE loss:0.0701\n","Epoch [43/100] - Iter[200/468], MSE loss:0.0713\n","Epoch [43/100] - Iter[300/468], MSE loss:0.0674\n","Epoch [43/100] - Iter[400/468], MSE loss:0.0719\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [44/100] - Iter[100/468], MSE loss:0.0706\n","Epoch [44/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [44/100] - Iter[300/468], MSE loss:0.0702\n","Epoch [44/100] - Iter[400/468], MSE loss:0.0716\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [45/100] - Iter[100/468], MSE loss:0.0689\n","Epoch [45/100] - Iter[200/468], MSE loss:0.0705\n","Epoch [45/100] - Iter[300/468], MSE loss:0.0705\n","Epoch [45/100] - Iter[400/468], MSE loss:0.0721\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [46/100] - Iter[100/468], MSE loss:0.0688\n","Epoch [46/100] - Iter[200/468], MSE loss:0.0765\n","Epoch [46/100] - Iter[300/468], MSE loss:0.0700\n","Epoch [46/100] - Iter[400/468], MSE loss:0.0656\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [47/100] - Iter[100/468], MSE loss:0.0673\n","Epoch [47/100] - Iter[200/468], MSE loss:0.0730\n","Epoch [47/100] - Iter[300/468], MSE loss:0.0690\n","Epoch [47/100] - Iter[400/468], MSE loss:0.0668\n","\n","Average MSE Loss on Test set: 0.0680\n","Saved Best Model\n","\n","Epoch [48/100] - Iter[100/468], MSE loss:0.0726\n","Epoch [48/100] - Iter[200/468], MSE loss:0.0680\n","Epoch [48/100] - Iter[300/468], MSE loss:0.0686\n","Epoch [48/100] - Iter[400/468], MSE loss:0.0690\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [49/100] - Iter[100/468], MSE loss:0.0669\n","Epoch [49/100] - Iter[200/468], MSE loss:0.0665\n","Epoch [49/100] - Iter[300/468], MSE loss:0.0696\n","Epoch [49/100] - Iter[400/468], MSE loss:0.0662\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [50/100] - Iter[100/468], MSE loss:0.0704\n","Epoch [50/100] - Iter[200/468], MSE loss:0.0669\n","Epoch [50/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [50/100] - Iter[400/468], MSE loss:0.0735\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [51/100] - Iter[100/468], MSE loss:0.0674\n","Epoch [51/100] - Iter[200/468], MSE loss:0.0671\n","Epoch [51/100] - Iter[300/468], MSE loss:0.0729\n","Epoch [51/100] - Iter[400/468], MSE loss:0.0697\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [52/100] - Iter[100/468], MSE loss:0.0698\n","Epoch [52/100] - Iter[200/468], MSE loss:0.0710\n","Epoch [52/100] - Iter[300/468], MSE loss:0.0682\n","Epoch [52/100] - Iter[400/468], MSE loss:0.0708\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [53/100] - Iter[100/468], MSE loss:0.0688\n","Epoch [53/100] - Iter[200/468], MSE loss:0.0670\n","Epoch [53/100] - Iter[300/468], MSE loss:0.0674\n","Epoch [53/100] - Iter[400/468], MSE loss:0.0655\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [54/100] - Iter[100/468], MSE loss:0.0688\n","Epoch [54/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [54/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [54/100] - Iter[400/468], MSE loss:0.0705\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [55/100] - Iter[100/468], MSE loss:0.0679\n","Epoch [55/100] - Iter[200/468], MSE loss:0.0676\n","Epoch [55/100] - Iter[300/468], MSE loss:0.0714\n","Epoch [55/100] - Iter[400/468], MSE loss:0.0687\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [56/100] - Iter[100/468], MSE loss:0.0706\n","Epoch [56/100] - Iter[200/468], MSE loss:0.0691\n","Epoch [56/100] - Iter[300/468], MSE loss:0.0671\n","Epoch [56/100] - Iter[400/468], MSE loss:0.0671\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [57/100] - Iter[100/468], MSE loss:0.0753\n","Epoch [57/100] - Iter[200/468], MSE loss:0.0695\n","Epoch [57/100] - Iter[300/468], MSE loss:0.0726\n","Epoch [57/100] - Iter[400/468], MSE loss:0.0670\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [58/100] - Iter[100/468], MSE loss:0.0743\n","Epoch [58/100] - Iter[200/468], MSE loss:0.0649\n","Epoch [58/100] - Iter[300/468], MSE loss:0.0701\n","Epoch [58/100] - Iter[400/468], MSE loss:0.0665\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [59/100] - Iter[100/468], MSE loss:0.0711\n","Epoch [59/100] - Iter[200/468], MSE loss:0.0701\n","Epoch [59/100] - Iter[300/468], MSE loss:0.0697\n","Epoch [59/100] - Iter[400/468], MSE loss:0.0692\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [60/100] - Iter[100/468], MSE loss:0.0697\n","Epoch [60/100] - Iter[200/468], MSE loss:0.0713\n","Epoch [60/100] - Iter[300/468], MSE loss:0.0705\n","Epoch [60/100] - Iter[400/468], MSE loss:0.0693\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [61/100] - Iter[100/468], MSE loss:0.0703\n","Epoch [61/100] - Iter[200/468], MSE loss:0.0741\n","Epoch [61/100] - Iter[300/468], MSE loss:0.0670\n","Epoch [61/100] - Iter[400/468], MSE loss:0.0666\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [62/100] - Iter[100/468], MSE loss:0.0721\n","Epoch [62/100] - Iter[200/468], MSE loss:0.0677\n","Epoch [62/100] - Iter[300/468], MSE loss:0.0681\n","Epoch [62/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [63/100] - Iter[100/468], MSE loss:0.0668\n","Epoch [63/100] - Iter[200/468], MSE loss:0.0698\n","Epoch [63/100] - Iter[300/468], MSE loss:0.0671\n","Epoch [63/100] - Iter[400/468], MSE loss:0.0695\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [64/100] - Iter[100/468], MSE loss:0.0722\n","Epoch [64/100] - Iter[200/468], MSE loss:0.0652\n","Epoch [64/100] - Iter[300/468], MSE loss:0.0669\n","Epoch [64/100] - Iter[400/468], MSE loss:0.0650\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [65/100] - Iter[100/468], MSE loss:0.0706\n","Epoch [65/100] - Iter[200/468], MSE loss:0.0693\n","Epoch [65/100] - Iter[300/468], MSE loss:0.0709\n","Epoch [65/100] - Iter[400/468], MSE loss:0.0642\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [66/100] - Iter[100/468], MSE loss:0.0672\n","Epoch [66/100] - Iter[200/468], MSE loss:0.0677\n","Epoch [66/100] - Iter[300/468], MSE loss:0.0725\n","Epoch [66/100] - Iter[400/468], MSE loss:0.0712\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [67/100] - Iter[100/468], MSE loss:0.0732\n","Epoch [67/100] - Iter[200/468], MSE loss:0.0685\n","Epoch [67/100] - Iter[300/468], MSE loss:0.0676\n","Epoch [67/100] - Iter[400/468], MSE loss:0.0690\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [68/100] - Iter[100/468], MSE loss:0.0741\n","Epoch [68/100] - Iter[200/468], MSE loss:0.0698\n","Epoch [68/100] - Iter[300/468], MSE loss:0.0682\n","Epoch [68/100] - Iter[400/468], MSE loss:0.0682\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [69/100] - Iter[100/468], MSE loss:0.0680\n","Epoch [69/100] - Iter[200/468], MSE loss:0.0710\n","Epoch [69/100] - Iter[300/468], MSE loss:0.0706\n","Epoch [69/100] - Iter[400/468], MSE loss:0.0704\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [70/100] - Iter[100/468], MSE loss:0.0672\n","Epoch [70/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [70/100] - Iter[300/468], MSE loss:0.0715\n","Epoch [70/100] - Iter[400/468], MSE loss:0.0692\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [71/100] - Iter[100/468], MSE loss:0.0699\n","Epoch [71/100] - Iter[200/468], MSE loss:0.0628\n","Epoch [71/100] - Iter[300/468], MSE loss:0.0711\n","Epoch [71/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [72/100] - Iter[100/468], MSE loss:0.0698\n","Epoch [72/100] - Iter[200/468], MSE loss:0.0770\n","Epoch [72/100] - Iter[300/468], MSE loss:0.0670\n","Epoch [72/100] - Iter[400/468], MSE loss:0.0704\n","\n","Average MSE Loss on Test set: 0.0677\n","Saved Best Model\n","\n","Epoch [73/100] - Iter[100/468], MSE loss:0.0688\n","Epoch [73/100] - Iter[200/468], MSE loss:0.0685\n","Epoch [73/100] - Iter[300/468], MSE loss:0.0717\n","Epoch [73/100] - Iter[400/468], MSE loss:0.0662\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [74/100] - Iter[100/468], MSE loss:0.0728\n","Epoch [74/100] - Iter[200/468], MSE loss:0.0718\n","Epoch [74/100] - Iter[300/468], MSE loss:0.0679\n","Epoch [74/100] - Iter[400/468], MSE loss:0.0734\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [75/100] - Iter[100/468], MSE loss:0.0705\n","Epoch [75/100] - Iter[200/468], MSE loss:0.0680\n","Epoch [75/100] - Iter[300/468], MSE loss:0.0739\n","Epoch [75/100] - Iter[400/468], MSE loss:0.0701\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [76/100] - Iter[100/468], MSE loss:0.0717\n","Epoch [76/100] - Iter[200/468], MSE loss:0.0669\n","Epoch [76/100] - Iter[300/468], MSE loss:0.0682\n","Epoch [76/100] - Iter[400/468], MSE loss:0.0678\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [77/100] - Iter[100/468], MSE loss:0.0706\n","Epoch [77/100] - Iter[200/468], MSE loss:0.0722\n","Epoch [77/100] - Iter[300/468], MSE loss:0.0691\n","Epoch [77/100] - Iter[400/468], MSE loss:0.0732\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [78/100] - Iter[100/468], MSE loss:0.0710\n","Epoch [78/100] - Iter[200/468], MSE loss:0.0705\n","Epoch [78/100] - Iter[300/468], MSE loss:0.0697\n","Epoch [78/100] - Iter[400/468], MSE loss:0.0730\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [79/100] - Iter[100/468], MSE loss:0.0733\n","Epoch [79/100] - Iter[200/468], MSE loss:0.0718\n","Epoch [79/100] - Iter[300/468], MSE loss:0.0712\n","Epoch [79/100] - Iter[400/468], MSE loss:0.0741\n","\n","Average MSE Loss on Test set: 0.0677\n","Saved Best Model\n","\n","Epoch [80/100] - Iter[100/468], MSE loss:0.0671\n","Epoch [80/100] - Iter[200/468], MSE loss:0.0658\n","Epoch [80/100] - Iter[300/468], MSE loss:0.0731\n","Epoch [80/100] - Iter[400/468], MSE loss:0.0696\n","\n","Average MSE Loss on Test set: 0.0679\n","Saved Best Model\n","\n","Epoch [81/100] - Iter[100/468], MSE loss:0.0724\n","Epoch [81/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [81/100] - Iter[300/468], MSE loss:0.0734\n","Epoch [81/100] - Iter[400/468], MSE loss:0.0657\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [82/100] - Iter[100/468], MSE loss:0.0635\n","Epoch [82/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [82/100] - Iter[300/468], MSE loss:0.0661\n","Epoch [82/100] - Iter[400/468], MSE loss:0.0711\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [83/100] - Iter[100/468], MSE loss:0.0730\n","Epoch [83/100] - Iter[200/468], MSE loss:0.0659\n","Epoch [83/100] - Iter[300/468], MSE loss:0.0695\n","Epoch [83/100] - Iter[400/468], MSE loss:0.0699\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [84/100] - Iter[100/468], MSE loss:0.0707\n","Epoch [84/100] - Iter[200/468], MSE loss:0.0691\n","Epoch [84/100] - Iter[300/468], MSE loss:0.0678\n","Epoch [84/100] - Iter[400/468], MSE loss:0.0728\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [85/100] - Iter[100/468], MSE loss:0.0659\n","Epoch [85/100] - Iter[200/468], MSE loss:0.0668\n","Epoch [85/100] - Iter[300/468], MSE loss:0.0684\n","Epoch [85/100] - Iter[400/468], MSE loss:0.0703\n","\n","Average MSE Loss on Test set: 0.0682\n","Saved Best Model\n","\n","Epoch [86/100] - Iter[100/468], MSE loss:0.0736\n","Epoch [86/100] - Iter[200/468], MSE loss:0.0632\n","Epoch [86/100] - Iter[300/468], MSE loss:0.0699\n","Epoch [86/100] - Iter[400/468], MSE loss:0.0727\n","\n","Average MSE Loss on Test set: 0.0677\n","Saved Best Model\n","\n","Epoch [87/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [87/100] - Iter[200/468], MSE loss:0.0691\n","Epoch [87/100] - Iter[300/468], MSE loss:0.0690\n","Epoch [87/100] - Iter[400/468], MSE loss:0.0681\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [88/100] - Iter[100/468], MSE loss:0.0730\n","Epoch [88/100] - Iter[200/468], MSE loss:0.0680\n","Epoch [88/100] - Iter[300/468], MSE loss:0.0704\n","Epoch [88/100] - Iter[400/468], MSE loss:0.0717\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [89/100] - Iter[100/468], MSE loss:0.0716\n","Epoch [89/100] - Iter[200/468], MSE loss:0.0682\n","Epoch [89/100] - Iter[300/468], MSE loss:0.0702\n","Epoch [89/100] - Iter[400/468], MSE loss:0.0689\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [90/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [90/100] - Iter[200/468], MSE loss:0.0703\n","Epoch [90/100] - Iter[300/468], MSE loss:0.0672\n","Epoch [90/100] - Iter[400/468], MSE loss:0.0706\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [91/100] - Iter[100/468], MSE loss:0.0681\n","Epoch [91/100] - Iter[200/468], MSE loss:0.0710\n","Epoch [91/100] - Iter[300/468], MSE loss:0.0720\n","Epoch [91/100] - Iter[400/468], MSE loss:0.0674\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [92/100] - Iter[100/468], MSE loss:0.0690\n","Epoch [92/100] - Iter[200/468], MSE loss:0.0686\n","Epoch [92/100] - Iter[300/468], MSE loss:0.0695\n","Epoch [92/100] - Iter[400/468], MSE loss:0.0726\n","\n","Average MSE Loss on Test set: 0.0677\n","Saved Best Model\n","\n","Epoch [93/100] - Iter[100/468], MSE loss:0.0651\n","Epoch [93/100] - Iter[200/468], MSE loss:0.0709\n","Epoch [93/100] - Iter[300/468], MSE loss:0.0699\n","Epoch [93/100] - Iter[400/468], MSE loss:0.0685\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [94/100] - Iter[100/468], MSE loss:0.0690\n","Epoch [94/100] - Iter[200/468], MSE loss:0.0712\n","Epoch [94/100] - Iter[300/468], MSE loss:0.0696\n","Epoch [94/100] - Iter[400/468], MSE loss:0.0662\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [95/100] - Iter[100/468], MSE loss:0.0691\n","Epoch [95/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [95/100] - Iter[300/468], MSE loss:0.0669\n","Epoch [95/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [96/100] - Iter[100/468], MSE loss:0.0699\n","Epoch [96/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [96/100] - Iter[300/468], MSE loss:0.0698\n","Epoch [96/100] - Iter[400/468], MSE loss:0.0723\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [97/100] - Iter[100/468], MSE loss:0.0699\n","Epoch [97/100] - Iter[200/468], MSE loss:0.0704\n","Epoch [97/100] - Iter[300/468], MSE loss:0.0701\n","Epoch [97/100] - Iter[400/468], MSE loss:0.0698\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [98/100] - Iter[100/468], MSE loss:0.0697\n","Epoch [98/100] - Iter[200/468], MSE loss:0.0655\n","Epoch [98/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [98/100] - Iter[400/468], MSE loss:0.0731\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [99/100] - Iter[100/468], MSE loss:0.0695\n","Epoch [99/100] - Iter[200/468], MSE loss:0.0699\n","Epoch [99/100] - Iter[300/468], MSE loss:0.0686\n","Epoch [99/100] - Iter[400/468], MSE loss:0.0748\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n","Epoch [100/100] - Iter[100/468], MSE loss:0.0694\n","Epoch [100/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [100/100] - Iter[300/468], MSE loss:0.0700\n","Epoch [100/100] - Iter[400/468], MSE loss:0.0697\n","\n","Average MSE Loss on Test set: 0.0678\n","Saved Best Model\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kvb2EXEyTTfh","colab_type":"text"},"source":["##Visualization of learned filters"]},{"cell_type":"markdown","metadata":{"id":"oiGOHXKcPk1p","colab_type":"text"},"source":["**when the training is done, visualize the weights of the first layer**\n"]},{"cell_type":"code","metadata":{"id":"H-TIQLdoR1fh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1570419417323,"user_tz":240,"elapsed":824,"user":{"displayName":"Chen Zhou","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAPQT2RBEruCVhGe2sNzB-3zrFI3wfjiLepdszV=s64","userId":"08552159914179628257"}},"outputId":"e1965f18-63c6-4561-a928-bdbc96335b98"},"source":["print('visualizing ' + arch)\n","\n","model = torch.load('./' + arch + '_best.pth')\n","\n","model_weights = model['encoder.2.weight'].cpu()      # w/o bias\n","\n","if arch != 'conv_AE':\n","    model_weights = model_weights.view(model_weights.shape[0], 1, 8, 8)  \n","\n","torch_scale = lambda x: (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n","model_weights_norm = torch_scale(model_weights)\n","\n","plt.figure(figsize=(12, 6))\n","plt.title(arch)\n","plt.imshow(np.transpose(torchvision.utils.make_grid(model_weights_norm).numpy(), (1, 2, 0)))\n","plt.axis('off')\n","plt.savefig(arch + '_filt.png')\n","plt.close('all')\n","print('figure saved')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["visualizing linear_AE\n","figure saved\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ApqxvXP9Trna","colab_type":"text"},"source":["#Learning specific structure"]},{"cell_type":"markdown","metadata":{"id":"W6HkAnkLTxpR","colab_type":"text"},"source":["##Sparse autoencoders"]},{"cell_type":"code","metadata":{"id":"mXcFE2BfTH4l","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0NEF6aHT0Ex","colab_type":"text"},"source":["#Applications"]},{"cell_type":"markdown","metadata":{"id":"p89798iuT8uo","colab_type":"text"},"source":["##Denoising autoencoders"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X67rYRbcXWlW"},"source":["**define the evaluation**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BctAbIN0XWlY","colab":{}},"source":["def evaluation(model, test_loader, epoch):\n","    total_loss = 0\n","    recons_loss = nn.MSELoss()\n","    model.eval()\n","    for batch_idx, (img, _) in enumerate(test_loader):\n","        if arch != 'conv_AE':\n","            img = img.view(img.size(0), -1)\n","        \n","        if arch == 'denoise_AE':\n","            noise_img = img * (1 - noise_ratio) + torch.rand(img.size()) * noise_ratio      # add noise onto input images\n","            noise_img = Variable(noise_img.cuda())\n","            img = Variable(img.cuda())\n","            out = model(noise_img)\n","        else:\n","            img = Variable(img.cuda())\n","            out = model(img)\n","        loss = recons_loss(out, img)\n","        total_loss += loss * img.size(0)\n","    avg_loss = total_loss / len(test_loader.dataset)\n","\n","    print('\\nAverage MSE Loss on Test set: {:.4f}'.format(avg_loss))\n","    if epoch % 10 == 0:\n","        # save input/output images\n","        if arch == 'denoise_AE':\n","            pic = to_img(noise_img.cpu().data)\n","            save_image(pic, './in_out_img/noise_input_{}.png'.format(epoch))\n","        pic = to_img(img.cpu().data)\n","        save_image(pic, './in_out_img/input_{}.png'.format(epoch))\n","        pic = to_img(out.cpu().data)\n","        save_image(pic, './in_out_img/recons_{}.png'.format(epoch))\n","    # save the best model with lowest mse loss    \n","    global best_val\n","    if avg_loss < best_val:\n","        best_val = avg_loss\n","        torch.save(model.state_dict(), './' + arch + '_best.pth')\n","    print('Saved Best Model\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkGcsq-YQpMo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1570370831128,"user_tz":240,"elapsed":946,"user":{"displayName":"Chen Zhou","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAPQT2RBEruCVhGe2sNzB-3zrFI3wfjiLepdszV=s64","userId":"08552159914179628257"}},"outputId":"a9b53814-eff8-487c-ca49-a8c1ec8613de"},"source":["import matplotlib.pyplot as plt\n","import gzip\n","import pickle\n","from PIL import Image\n","import cv2\n","dataset = MNIST('./data', train=True, download=True,transform=None)\n","# fix random seed for reproducible results\n","seed = 0\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","# \n","filename = [\n","[\"training_images\",\"./data/MNIST/raw/train-images-idx3-ubyte.gz\"],\n","[\"test_images\",\"./data/MNIST/raw/t10k-images-idx3-ubyte.gz\"],\n","[\"training_labels\",\"./data/MNIST/raw/train-labels-idx1-ubyte.gz\"],\n","[\"test_labels\",\"./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\"]\n","]\n","\n","# load downloaded .gz files to and save images as numpy arrays \n","def save_mnist():\n","    mnist = {}\n","    for name in filename[:2]:\n","        with gzip.open(name[1], 'rb') as f:\n","            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n","    # for name in filename[-2:]:\n","    #     with gzip.open(name[1], 'rb') as f:\n","    #         mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n","    with open(\"mnist.pkl\", 'wb') as f:\n","        pickle.dump(mnist,f)\n","    print(\"Save complete.\")\n","\n","# save_mnist()\n","\n","with open(\"mnist.pkl\",'rb') as f:\n","    mnist = pickle.load(f)\n","    mnist_train_X = mnist[\"training_images\"]  # 60000, 784\n","    mnist_train_X = mnist[\"test_images\"]  # 10000, 784\n","\n","# TODO: define idx\n","idx = 0\n","img = mnist_train_X[idx].reshape(28,28)  / 255.\n","\n","plt.figure()\n","plt.imshow(img, cmap='gray')\n","plt.axis('off')\n","\n","\n","Image.fromarray(img).convert(\"L\").save(\"clean_input.jpg\")\n","# plt.savefig('clean_input.png')\n","\n","noise = torch.rand(torch.Size([28, 28])).numpy()\n","plt.figure()\n","plt.imshow(noise,cmap='gray')\n","plt.axis('off')\n","Image.fromarray(noise).convert(\"L\").save(\"clean_input.jpg\")\n","# plt.savefig('rand_noise.png')\n","\n","corrupt_img = img * (1 - noise_ratio) + noise * noise_ratio \n","plt.figure()\n","plt.imshow(corrupt_img,cmap='gray')\n","plt.axis('off')\n","Image.fromarray(corrupt_img).convert(\"L\").save(\"clean_input.jpg\")\n","# plt.savefig('corrupt_input.png')\n","\n","\n","blurry_img = cv2.GaussianBlur(img,(3,3),0)\n","plt.figure()\n","plt.imshow(blurry_img,cmap='gray')\n","plt.axis('off')\n","plt.savefig('blurry_out.png')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhNghhNghhNghhNghhNghhNghhNghxP8AWOQ1MdJ32I8AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4wvRAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB5z4D7gBnVgP4wieAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s6zujIYdXWlk"},"source":["**start training**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B9hauaGcXWlk","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1570056642804,"user_tz":240,"elapsed":1370700,"user":{"displayName":"Chen Zhou","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAPQT2RBEruCVhGe2sNzB-3zrFI3wfjiLepdszV=s64","userId":"08552159914179628257"}},"outputId":"4d318cd8-8fa4-45e0-baef-8fc1ab6b6d96"},"source":["\n","# fix random seed for reproducible results\n","seed = 0\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","\n","# define method to save output images \n","def to_img(x):\n","    x = 0.5 * (x + 1)   # Tanh activation squashes the output to range [-1, 1] \n","    x = x.clamp(0, 1)   \n","    x = x.view(x.size(0), 1, 28, 28)    # reshae the vectorized output to grayscale images\n","    return x\n","\n","# define the transformation applied to MNIST images\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# train/test loader\n","train_loader = DataLoader(MNIST('./data', train=True, download=True, \n","                          transform=transform), batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(MNIST('./data', train=False, \n","                          transform=transform), batch_size=batch_size, shuffle=False)\n","\n","\n","if arch == 'nonlinear_AE':  \n","    model = nonlinear_AE().cuda()\n","    # model.load_state_dict(torch.load('./.pth'))\n","elif arch == 'linear_AE':\n","    model = linear_AE().cuda()\n","elif arch == 'sparse_AE':\n","    model = sparse_AE().cuda()\n","elif arch == 'conv_AE':\n","    model = conv_AE().cuda()\n","##TODO: denoising autoencoders    \n","elif arch == 'denoise_AE':\n","    model = linear_AE().cuda()\n","print('training {} model'.format(arch))\n","\n","# use Mean Squared Error loss for reconstruction\n","recons_loss = nn.MSELoss()\n","\n","# define the optimizer\n","# optimizer =  torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","print('Adam optimizer')\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","for epoch in range(num_epochs):\n","    model.train()     # train mode\n","    #  no use of labels\n","    for batch_idx, (img, _) in enumerate(train_loader):\n","        # show the first image in the 'batch_id'-th batch\n","        # plt.imshow((data[0].numpy().copy().squeeze(0)), cmap='gray')\n","        if arch != 'conv_AE':\n","            # strech images into column vectors if not training convolutional AE\n","            img = img.view(img.size(0), -1)\n","        # ===================forward=====================\n","        # output reconstructed images\n","        if arch == 'denoise_AE':\n","            noise_img = img * (1 - noise_ratio) + torch.rand(img.size()) * noise_ratio      # add noise onto input images\n","            noise_img = Variable(noise_img.cuda())\n","            img = Variable(img).cuda()\n","            out = model(noise_img)\n","        else:\n","            img = Variable(img).cuda()\n","            out = model(img)\n","        \n","        if arch == 'sparse_AE':  # if sparse autoencoder, use MSE loss and L1 loss\n","            l1_loss = sparse_loss(model, img)\n","            mse_loss = recons_loss(out, img)\n","            loss = mse_loss + sparse_reg * l1_loss\n","        else: # compute the MSE loss for models w/o regularization\n","            loss = recons_loss(out, img)\n","        # ===================backward====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # ===================log========================\n","        if (batch_idx + 1) % 100 == 0:\n","            if arch == 'sparse_AE':\n","                print('Epoch [{}/{}] - Iter[{}/{}], Total loss:{:.4f}, MSE loss:{:.4f}, Sparse loss:{:.4f}'.format(\n","                    epoch + 1, num_epochs, batch_idx + 1, len(train_loader.dataset) // batch_size, loss.item(), mse_loss.item(), l1_loss.item()))\n","            else:\n","                print('Epoch [{}/{}] - Iter[{}/{}], MSE loss:{:.4f}'.format(\n","                    epoch + 1, num_epochs, batch_idx + 1, len(train_loader.dataset) // batch_size, loss.item()))\n","    # evaluation on test set\n","    evaluation(model, test_loader, epoch)\n","    # ===================log========================\n","    # print('epoch [{}/{}], MSE loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n","    \n","# save the model to file\n","torch.save(model.state_dict(), './' + arch + '_final.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["9920512it [00:01, 9560760.33it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/28881 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 137156.00it/s]           \n","  0%|          | 0/1648877 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["1654784it [00:00, 2200155.31it/s]                            \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 53176.51it/s]            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","Processing...\n","Done!\n","training denoise_AE model\n","Adam optimizer\n","Epoch [1/100] - Iter[100/468], MSE loss:0.1666\n","Epoch [1/100] - Iter[200/468], MSE loss:0.1267\n","Epoch [1/100] - Iter[300/468], MSE loss:0.1089\n","Epoch [1/100] - Iter[400/468], MSE loss:0.0973\n","\n","Average MSE Loss on Test set: 0.0924\n","Saved Best Model\n","\n","Epoch [2/100] - Iter[100/468], MSE loss:0.0933\n","Epoch [2/100] - Iter[200/468], MSE loss:0.0845\n","Epoch [2/100] - Iter[300/468], MSE loss:0.0809\n","Epoch [2/100] - Iter[400/468], MSE loss:0.0805\n","\n","Average MSE Loss on Test set: 0.0751\n","Saved Best Model\n","\n","Epoch [3/100] - Iter[100/468], MSE loss:0.0718\n","Epoch [3/100] - Iter[200/468], MSE loss:0.0728\n","Epoch [3/100] - Iter[300/468], MSE loss:0.0744\n","Epoch [3/100] - Iter[400/468], MSE loss:0.0730\n","\n","Average MSE Loss on Test set: 0.0713\n","Saved Best Model\n","\n","Epoch [4/100] - Iter[100/468], MSE loss:0.0737\n","Epoch [4/100] - Iter[200/468], MSE loss:0.0721\n","Epoch [4/100] - Iter[300/468], MSE loss:0.0745\n","Epoch [4/100] - Iter[400/468], MSE loss:0.0771\n","\n","Average MSE Loss on Test set: 0.0713\n","Saved Best Model\n","\n","Epoch [5/100] - Iter[100/468], MSE loss:0.0758\n","Epoch [5/100] - Iter[200/468], MSE loss:0.0734\n","Epoch [5/100] - Iter[300/468], MSE loss:0.0760\n","Epoch [5/100] - Iter[400/468], MSE loss:0.0748\n","\n","Average MSE Loss on Test set: 0.0712\n","Saved Best Model\n","\n","Epoch [6/100] - Iter[100/468], MSE loss:0.0716\n","Epoch [6/100] - Iter[200/468], MSE loss:0.0760\n","Epoch [6/100] - Iter[300/468], MSE loss:0.0733\n","Epoch [6/100] - Iter[400/468], MSE loss:0.0723\n","\n","Average MSE Loss on Test set: 0.0713\n","Saved Best Model\n","\n","Epoch [7/100] - Iter[100/468], MSE loss:0.0744\n","Epoch [7/100] - Iter[200/468], MSE loss:0.0745\n","Epoch [7/100] - Iter[300/468], MSE loss:0.0732\n","Epoch [7/100] - Iter[400/468], MSE loss:0.0747\n","\n","Average MSE Loss on Test set: 0.0702\n","Saved Best Model\n","\n","Epoch [8/100] - Iter[100/468], MSE loss:0.0734\n","Epoch [8/100] - Iter[200/468], MSE loss:0.0692\n","Epoch [8/100] - Iter[300/468], MSE loss:0.0686\n","Epoch [8/100] - Iter[400/468], MSE loss:0.0698\n","\n","Average MSE Loss on Test set: 0.0699\n","Saved Best Model\n","\n","Epoch [9/100] - Iter[100/468], MSE loss:0.0719\n","Epoch [9/100] - Iter[200/468], MSE loss:0.0730\n","Epoch [9/100] - Iter[300/468], MSE loss:0.0712\n","Epoch [9/100] - Iter[400/468], MSE loss:0.0704\n","\n","Average MSE Loss on Test set: 0.0699\n","Saved Best Model\n","\n","Epoch [10/100] - Iter[100/468], MSE loss:0.0746\n","Epoch [10/100] - Iter[200/468], MSE loss:0.0721\n","Epoch [10/100] - Iter[300/468], MSE loss:0.0715\n","Epoch [10/100] - Iter[400/468], MSE loss:0.0747\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [11/100] - Iter[100/468], MSE loss:0.0706\n","Epoch [11/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [11/100] - Iter[300/468], MSE loss:0.0677\n","Epoch [11/100] - Iter[400/468], MSE loss:0.0754\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [12/100] - Iter[100/468], MSE loss:0.0698\n","Epoch [12/100] - Iter[200/468], MSE loss:0.0726\n","Epoch [12/100] - Iter[300/468], MSE loss:0.0668\n","Epoch [12/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [13/100] - Iter[100/468], MSE loss:0.0737\n","Epoch [13/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [13/100] - Iter[300/468], MSE loss:0.0734\n","Epoch [13/100] - Iter[400/468], MSE loss:0.0689\n","\n","Average MSE Loss on Test set: 0.0699\n","Saved Best Model\n","\n","Epoch [14/100] - Iter[100/468], MSE loss:0.0754\n","Epoch [14/100] - Iter[200/468], MSE loss:0.0700\n","Epoch [14/100] - Iter[300/468], MSE loss:0.0723\n","Epoch [14/100] - Iter[400/468], MSE loss:0.0689\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [15/100] - Iter[100/468], MSE loss:0.0729\n","Epoch [15/100] - Iter[200/468], MSE loss:0.0708\n","Epoch [15/100] - Iter[300/468], MSE loss:0.0754\n","Epoch [15/100] - Iter[400/468], MSE loss:0.0740\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [16/100] - Iter[100/468], MSE loss:0.0709\n","Epoch [16/100] - Iter[200/468], MSE loss:0.0716\n","Epoch [16/100] - Iter[300/468], MSE loss:0.0694\n","Epoch [16/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [17/100] - Iter[100/468], MSE loss:0.0675\n","Epoch [17/100] - Iter[200/468], MSE loss:0.0740\n","Epoch [17/100] - Iter[300/468], MSE loss:0.0704\n","Epoch [17/100] - Iter[400/468], MSE loss:0.0703\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [18/100] - Iter[100/468], MSE loss:0.0690\n","Epoch [18/100] - Iter[200/468], MSE loss:0.0708\n","Epoch [18/100] - Iter[300/468], MSE loss:0.0717\n","Epoch [18/100] - Iter[400/468], MSE loss:0.0744\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [19/100] - Iter[100/468], MSE loss:0.0678\n","Epoch [19/100] - Iter[200/468], MSE loss:0.0737\n","Epoch [19/100] - Iter[300/468], MSE loss:0.0741\n","Epoch [19/100] - Iter[400/468], MSE loss:0.0743\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [20/100] - Iter[100/468], MSE loss:0.0704\n","Epoch [20/100] - Iter[200/468], MSE loss:0.0745\n","Epoch [20/100] - Iter[300/468], MSE loss:0.0745\n","Epoch [20/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [21/100] - Iter[100/468], MSE loss:0.0716\n","Epoch [21/100] - Iter[200/468], MSE loss:0.0745\n","Epoch [21/100] - Iter[300/468], MSE loss:0.0662\n","Epoch [21/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [22/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [22/100] - Iter[200/468], MSE loss:0.0695\n","Epoch [22/100] - Iter[300/468], MSE loss:0.0711\n","Epoch [22/100] - Iter[400/468], MSE loss:0.0683\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [23/100] - Iter[100/468], MSE loss:0.0720\n","Epoch [23/100] - Iter[200/468], MSE loss:0.0724\n","Epoch [23/100] - Iter[300/468], MSE loss:0.0714\n","Epoch [23/100] - Iter[400/468], MSE loss:0.0692\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [24/100] - Iter[100/468], MSE loss:0.0728\n","Epoch [24/100] - Iter[200/468], MSE loss:0.0688\n","Epoch [24/100] - Iter[300/468], MSE loss:0.0681\n","Epoch [24/100] - Iter[400/468], MSE loss:0.0712\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [25/100] - Iter[100/468], MSE loss:0.0717\n","Epoch [25/100] - Iter[200/468], MSE loss:0.0692\n","Epoch [25/100] - Iter[300/468], MSE loss:0.0676\n","Epoch [25/100] - Iter[400/468], MSE loss:0.0725\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [26/100] - Iter[100/468], MSE loss:0.0723\n","Epoch [26/100] - Iter[200/468], MSE loss:0.0750\n","Epoch [26/100] - Iter[300/468], MSE loss:0.0730\n","Epoch [26/100] - Iter[400/468], MSE loss:0.0714\n","\n","Average MSE Loss on Test set: 0.0698\n","Saved Best Model\n","\n","Epoch [27/100] - Iter[100/468], MSE loss:0.0685\n","Epoch [27/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [27/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [27/100] - Iter[400/468], MSE loss:0.0701\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [28/100] - Iter[100/468], MSE loss:0.0731\n","Epoch [28/100] - Iter[200/468], MSE loss:0.0730\n","Epoch [28/100] - Iter[300/468], MSE loss:0.0720\n","Epoch [28/100] - Iter[400/468], MSE loss:0.0718\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [29/100] - Iter[100/468], MSE loss:0.0716\n","Epoch [29/100] - Iter[200/468], MSE loss:0.0763\n","Epoch [29/100] - Iter[300/468], MSE loss:0.0678\n","Epoch [29/100] - Iter[400/468], MSE loss:0.0731\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [30/100] - Iter[100/468], MSE loss:0.0703\n","Epoch [30/100] - Iter[200/468], MSE loss:0.0724\n","Epoch [30/100] - Iter[300/468], MSE loss:0.0722\n","Epoch [30/100] - Iter[400/468], MSE loss:0.0722\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [31/100] - Iter[100/468], MSE loss:0.0702\n","Epoch [31/100] - Iter[200/468], MSE loss:0.0708\n","Epoch [31/100] - Iter[300/468], MSE loss:0.0725\n","Epoch [31/100] - Iter[400/468], MSE loss:0.0734\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [32/100] - Iter[100/468], MSE loss:0.0722\n","Epoch [32/100] - Iter[200/468], MSE loss:0.0713\n","Epoch [32/100] - Iter[300/468], MSE loss:0.0696\n","Epoch [32/100] - Iter[400/468], MSE loss:0.0749\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [33/100] - Iter[100/468], MSE loss:0.0714\n","Epoch [33/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [33/100] - Iter[300/468], MSE loss:0.0743\n","Epoch [33/100] - Iter[400/468], MSE loss:0.0756\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [34/100] - Iter[100/468], MSE loss:0.0721\n","Epoch [34/100] - Iter[200/468], MSE loss:0.0715\n","Epoch [34/100] - Iter[300/468], MSE loss:0.0672\n","Epoch [34/100] - Iter[400/468], MSE loss:0.0693\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [35/100] - Iter[100/468], MSE loss:0.0707\n","Epoch [35/100] - Iter[200/468], MSE loss:0.0662\n","Epoch [35/100] - Iter[300/468], MSE loss:0.0742\n","Epoch [35/100] - Iter[400/468], MSE loss:0.0741\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [36/100] - Iter[100/468], MSE loss:0.0681\n","Epoch [36/100] - Iter[200/468], MSE loss:0.0759\n","Epoch [36/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [36/100] - Iter[400/468], MSE loss:0.0744\n","\n","Average MSE Loss on Test set: 0.0697\n","Saved Best Model\n","\n","Epoch [37/100] - Iter[100/468], MSE loss:0.0684\n","Epoch [37/100] - Iter[200/468], MSE loss:0.0695\n","Epoch [37/100] - Iter[300/468], MSE loss:0.0715\n","Epoch [37/100] - Iter[400/468], MSE loss:0.0682\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [38/100] - Iter[100/468], MSE loss:0.0745\n","Epoch [38/100] - Iter[200/468], MSE loss:0.0713\n","Epoch [38/100] - Iter[300/468], MSE loss:0.0729\n","Epoch [38/100] - Iter[400/468], MSE loss:0.0706\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [39/100] - Iter[100/468], MSE loss:0.0718\n","Epoch [39/100] - Iter[200/468], MSE loss:0.0679\n","Epoch [39/100] - Iter[300/468], MSE loss:0.0709\n","Epoch [39/100] - Iter[400/468], MSE loss:0.0724\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [40/100] - Iter[100/468], MSE loss:0.0707\n","Epoch [40/100] - Iter[200/468], MSE loss:0.0682\n","Epoch [40/100] - Iter[300/468], MSE loss:0.0704\n","Epoch [40/100] - Iter[400/468], MSE loss:0.0708\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [41/100] - Iter[100/468], MSE loss:0.0759\n","Epoch [41/100] - Iter[200/468], MSE loss:0.0701\n","Epoch [41/100] - Iter[300/468], MSE loss:0.0749\n","Epoch [41/100] - Iter[400/468], MSE loss:0.0708\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [42/100] - Iter[100/468], MSE loss:0.0676\n","Epoch [42/100] - Iter[200/468], MSE loss:0.0776\n","Epoch [42/100] - Iter[300/468], MSE loss:0.0748\n","Epoch [42/100] - Iter[400/468], MSE loss:0.0744\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [43/100] - Iter[100/468], MSE loss:0.0709\n","Epoch [43/100] - Iter[200/468], MSE loss:0.0721\n","Epoch [43/100] - Iter[300/468], MSE loss:0.0684\n","Epoch [43/100] - Iter[400/468], MSE loss:0.0742\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [44/100] - Iter[100/468], MSE loss:0.0652\n","Epoch [44/100] - Iter[200/468], MSE loss:0.0733\n","Epoch [44/100] - Iter[300/468], MSE loss:0.0727\n","Epoch [44/100] - Iter[400/468], MSE loss:0.0712\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [45/100] - Iter[100/468], MSE loss:0.0702\n","Epoch [45/100] - Iter[200/468], MSE loss:0.0737\n","Epoch [45/100] - Iter[300/468], MSE loss:0.0711\n","Epoch [45/100] - Iter[400/468], MSE loss:0.0772\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [46/100] - Iter[100/468], MSE loss:0.0717\n","Epoch [46/100] - Iter[200/468], MSE loss:0.0738\n","Epoch [46/100] - Iter[300/468], MSE loss:0.0721\n","Epoch [46/100] - Iter[400/468], MSE loss:0.0738\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [47/100] - Iter[100/468], MSE loss:0.0731\n","Epoch [47/100] - Iter[200/468], MSE loss:0.0722\n","Epoch [47/100] - Iter[300/468], MSE loss:0.0742\n","Epoch [47/100] - Iter[400/468], MSE loss:0.0666\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [48/100] - Iter[100/468], MSE loss:0.0679\n","Epoch [48/100] - Iter[200/468], MSE loss:0.0695\n","Epoch [48/100] - Iter[300/468], MSE loss:0.0737\n","Epoch [48/100] - Iter[400/468], MSE loss:0.0756\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [49/100] - Iter[100/468], MSE loss:0.0713\n","Epoch [49/100] - Iter[200/468], MSE loss:0.0713\n","Epoch [49/100] - Iter[300/468], MSE loss:0.0711\n","Epoch [49/100] - Iter[400/468], MSE loss:0.0724\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [50/100] - Iter[100/468], MSE loss:0.0730\n","Epoch [50/100] - Iter[200/468], MSE loss:0.0740\n","Epoch [50/100] - Iter[300/468], MSE loss:0.0751\n","Epoch [50/100] - Iter[400/468], MSE loss:0.0679\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [51/100] - Iter[100/468], MSE loss:0.0728\n","Epoch [51/100] - Iter[200/468], MSE loss:0.0689\n","Epoch [51/100] - Iter[300/468], MSE loss:0.0702\n","Epoch [51/100] - Iter[400/468], MSE loss:0.0694\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [52/100] - Iter[100/468], MSE loss:0.0674\n","Epoch [52/100] - Iter[200/468], MSE loss:0.0703\n","Epoch [52/100] - Iter[300/468], MSE loss:0.0739\n","Epoch [52/100] - Iter[400/468], MSE loss:0.0700\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [53/100] - Iter[100/468], MSE loss:0.0720\n","Epoch [53/100] - Iter[200/468], MSE loss:0.0705\n","Epoch [53/100] - Iter[300/468], MSE loss:0.0720\n","Epoch [53/100] - Iter[400/468], MSE loss:0.0714\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [54/100] - Iter[100/468], MSE loss:0.0731\n","Epoch [54/100] - Iter[200/468], MSE loss:0.0735\n","Epoch [54/100] - Iter[300/468], MSE loss:0.0764\n","Epoch [54/100] - Iter[400/468], MSE loss:0.0703\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [55/100] - Iter[100/468], MSE loss:0.0710\n","Epoch [55/100] - Iter[200/468], MSE loss:0.0755\n","Epoch [55/100] - Iter[300/468], MSE loss:0.0678\n","Epoch [55/100] - Iter[400/468], MSE loss:0.0720\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [56/100] - Iter[100/468], MSE loss:0.0739\n","Epoch [56/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [56/100] - Iter[300/468], MSE loss:0.0686\n","Epoch [56/100] - Iter[400/468], MSE loss:0.0732\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [57/100] - Iter[100/468], MSE loss:0.0729\n","Epoch [57/100] - Iter[200/468], MSE loss:0.0703\n","Epoch [57/100] - Iter[300/468], MSE loss:0.0687\n","Epoch [57/100] - Iter[400/468], MSE loss:0.0682\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [58/100] - Iter[100/468], MSE loss:0.0735\n","Epoch [58/100] - Iter[200/468], MSE loss:0.0737\n","Epoch [58/100] - Iter[300/468], MSE loss:0.0731\n","Epoch [58/100] - Iter[400/468], MSE loss:0.0728\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [59/100] - Iter[100/468], MSE loss:0.0708\n","Epoch [59/100] - Iter[200/468], MSE loss:0.0701\n","Epoch [59/100] - Iter[300/468], MSE loss:0.0732\n","Epoch [59/100] - Iter[400/468], MSE loss:0.0726\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [60/100] - Iter[100/468], MSE loss:0.0677\n","Epoch [60/100] - Iter[200/468], MSE loss:0.0699\n","Epoch [60/100] - Iter[300/468], MSE loss:0.0711\n","Epoch [60/100] - Iter[400/468], MSE loss:0.0733\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [61/100] - Iter[100/468], MSE loss:0.0739\n","Epoch [61/100] - Iter[200/468], MSE loss:0.0716\n","Epoch [61/100] - Iter[300/468], MSE loss:0.0729\n","Epoch [61/100] - Iter[400/468], MSE loss:0.0675\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [62/100] - Iter[100/468], MSE loss:0.0698\n","Epoch [62/100] - Iter[200/468], MSE loss:0.0678\n","Epoch [62/100] - Iter[300/468], MSE loss:0.0710\n","Epoch [62/100] - Iter[400/468], MSE loss:0.0691\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [63/100] - Iter[100/468], MSE loss:0.0680\n","Epoch [63/100] - Iter[200/468], MSE loss:0.0731\n","Epoch [63/100] - Iter[300/468], MSE loss:0.0705\n","Epoch [63/100] - Iter[400/468], MSE loss:0.0720\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [64/100] - Iter[100/468], MSE loss:0.0703\n","Epoch [64/100] - Iter[200/468], MSE loss:0.0723\n","Epoch [64/100] - Iter[300/468], MSE loss:0.0671\n","Epoch [64/100] - Iter[400/468], MSE loss:0.0679\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [65/100] - Iter[100/468], MSE loss:0.0742\n","Epoch [65/100] - Iter[200/468], MSE loss:0.0696\n","Epoch [65/100] - Iter[300/468], MSE loss:0.0676\n","Epoch [65/100] - Iter[400/468], MSE loss:0.0692\n","\n","Average MSE Loss on Test set: 0.0694\n","Saved Best Model\n","\n","Epoch [66/100] - Iter[100/468], MSE loss:0.0675\n","Epoch [66/100] - Iter[200/468], MSE loss:0.0692\n","Epoch [66/100] - Iter[300/468], MSE loss:0.0680\n","Epoch [66/100] - Iter[400/468], MSE loss:0.0697\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [67/100] - Iter[100/468], MSE loss:0.0702\n","Epoch [67/100] - Iter[200/468], MSE loss:0.0744\n","Epoch [67/100] - Iter[300/468], MSE loss:0.0696\n","Epoch [67/100] - Iter[400/468], MSE loss:0.0711\n","\n","Average MSE Loss on Test set: 0.0694\n","Saved Best Model\n","\n","Epoch [68/100] - Iter[100/468], MSE loss:0.0717\n","Epoch [68/100] - Iter[200/468], MSE loss:0.0670\n","Epoch [68/100] - Iter[300/468], MSE loss:0.0689\n","Epoch [68/100] - Iter[400/468], MSE loss:0.0748\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [69/100] - Iter[100/468], MSE loss:0.0754\n","Epoch [69/100] - Iter[200/468], MSE loss:0.0705\n","Epoch [69/100] - Iter[300/468], MSE loss:0.0671\n","Epoch [69/100] - Iter[400/468], MSE loss:0.0743\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [70/100] - Iter[100/468], MSE loss:0.0708\n","Epoch [70/100] - Iter[200/468], MSE loss:0.0733\n","Epoch [70/100] - Iter[300/468], MSE loss:0.0710\n","Epoch [70/100] - Iter[400/468], MSE loss:0.0706\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [71/100] - Iter[100/468], MSE loss:0.0692\n","Epoch [71/100] - Iter[200/468], MSE loss:0.0744\n","Epoch [71/100] - Iter[300/468], MSE loss:0.0738\n","Epoch [71/100] - Iter[400/468], MSE loss:0.0681\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [72/100] - Iter[100/468], MSE loss:0.0737\n","Epoch [72/100] - Iter[200/468], MSE loss:0.0707\n","Epoch [72/100] - Iter[300/468], MSE loss:0.0702\n","Epoch [72/100] - Iter[400/468], MSE loss:0.0690\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [73/100] - Iter[100/468], MSE loss:0.0699\n","Epoch [73/100] - Iter[200/468], MSE loss:0.0689\n","Epoch [73/100] - Iter[300/468], MSE loss:0.0725\n","Epoch [73/100] - Iter[400/468], MSE loss:0.0713\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [74/100] - Iter[100/468], MSE loss:0.0712\n","Epoch [74/100] - Iter[200/468], MSE loss:0.0690\n","Epoch [74/100] - Iter[300/468], MSE loss:0.0724\n","Epoch [74/100] - Iter[400/468], MSE loss:0.0701\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [75/100] - Iter[100/468], MSE loss:0.0734\n","Epoch [75/100] - Iter[200/468], MSE loss:0.0766\n","Epoch [75/100] - Iter[300/468], MSE loss:0.0716\n","Epoch [75/100] - Iter[400/468], MSE loss:0.0684\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [76/100] - Iter[100/468], MSE loss:0.0710\n","Epoch [76/100] - Iter[200/468], MSE loss:0.0683\n","Epoch [76/100] - Iter[300/468], MSE loss:0.0733\n","Epoch [76/100] - Iter[400/468], MSE loss:0.0750\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [77/100] - Iter[100/468], MSE loss:0.0711\n","Epoch [77/100] - Iter[200/468], MSE loss:0.0700\n","Epoch [77/100] - Iter[300/468], MSE loss:0.0775\n","Epoch [77/100] - Iter[400/468], MSE loss:0.0733\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [78/100] - Iter[100/468], MSE loss:0.0737\n","Epoch [78/100] - Iter[200/468], MSE loss:0.0715\n","Epoch [78/100] - Iter[300/468], MSE loss:0.0675\n","Epoch [78/100] - Iter[400/468], MSE loss:0.0734\n","\n","Average MSE Loss on Test set: 0.0696\n","Saved Best Model\n","\n","Epoch [79/100] - Iter[100/468], MSE loss:0.0735\n","Epoch [79/100] - Iter[200/468], MSE loss:0.0721\n","Epoch [79/100] - Iter[300/468], MSE loss:0.0740\n","Epoch [79/100] - Iter[400/468], MSE loss:0.0734\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [80/100] - Iter[100/468], MSE loss:0.0729\n","Epoch [80/100] - Iter[200/468], MSE loss:0.0695\n","Epoch [80/100] - Iter[300/468], MSE loss:0.0715\n","Epoch [80/100] - Iter[400/468], MSE loss:0.0717\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [81/100] - Iter[100/468], MSE loss:0.0740\n","Epoch [81/100] - Iter[200/468], MSE loss:0.0747\n","Epoch [81/100] - Iter[300/468], MSE loss:0.0693\n","Epoch [81/100] - Iter[400/468], MSE loss:0.0772\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [82/100] - Iter[100/468], MSE loss:0.0743\n","Epoch [82/100] - Iter[200/468], MSE loss:0.0737\n","Epoch [82/100] - Iter[300/468], MSE loss:0.0720\n","Epoch [82/100] - Iter[400/468], MSE loss:0.0699\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [83/100] - Iter[100/468], MSE loss:0.0689\n","Epoch [83/100] - Iter[200/468], MSE loss:0.0726\n","Epoch [83/100] - Iter[300/468], MSE loss:0.0739\n","Epoch [83/100] - Iter[400/468], MSE loss:0.0687\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [84/100] - Iter[100/468], MSE loss:0.0718\n","Epoch [84/100] - Iter[200/468], MSE loss:0.0715\n","Epoch [84/100] - Iter[300/468], MSE loss:0.0695\n","Epoch [84/100] - Iter[400/468], MSE loss:0.0748\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [85/100] - Iter[100/468], MSE loss:0.0720\n","Epoch [85/100] - Iter[200/468], MSE loss:0.0710\n","Epoch [85/100] - Iter[300/468], MSE loss:0.0691\n","Epoch [85/100] - Iter[400/468], MSE loss:0.0711\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [86/100] - Iter[100/468], MSE loss:0.0708\n","Epoch [86/100] - Iter[200/468], MSE loss:0.0747\n","Epoch [86/100] - Iter[300/468], MSE loss:0.0766\n","Epoch [86/100] - Iter[400/468], MSE loss:0.0663\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [87/100] - Iter[100/468], MSE loss:0.0710\n","Epoch [87/100] - Iter[200/468], MSE loss:0.0732\n","Epoch [87/100] - Iter[300/468], MSE loss:0.0720\n","Epoch [87/100] - Iter[400/468], MSE loss:0.0742\n","\n","Average MSE Loss on Test set: 0.0694\n","Saved Best Model\n","\n","Epoch [88/100] - Iter[100/468], MSE loss:0.0689\n","Epoch [88/100] - Iter[200/468], MSE loss:0.0697\n","Epoch [88/100] - Iter[300/468], MSE loss:0.0674\n","Epoch [88/100] - Iter[400/468], MSE loss:0.0699\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [89/100] - Iter[100/468], MSE loss:0.0730\n","Epoch [89/100] - Iter[200/468], MSE loss:0.0655\n","Epoch [89/100] - Iter[300/468], MSE loss:0.0703\n","Epoch [89/100] - Iter[400/468], MSE loss:0.0744\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [90/100] - Iter[100/468], MSE loss:0.0714\n","Epoch [90/100] - Iter[200/468], MSE loss:0.0692\n","Epoch [90/100] - Iter[300/468], MSE loss:0.0724\n","Epoch [90/100] - Iter[400/468], MSE loss:0.0695\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [91/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [91/100] - Iter[200/468], MSE loss:0.0674\n","Epoch [91/100] - Iter[300/468], MSE loss:0.0694\n","Epoch [91/100] - Iter[400/468], MSE loss:0.0691\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [92/100] - Iter[100/468], MSE loss:0.0733\n","Epoch [92/100] - Iter[200/468], MSE loss:0.0724\n","Epoch [92/100] - Iter[300/468], MSE loss:0.0715\n","Epoch [92/100] - Iter[400/468], MSE loss:0.0728\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [93/100] - Iter[100/468], MSE loss:0.0703\n","Epoch [93/100] - Iter[200/468], MSE loss:0.0681\n","Epoch [93/100] - Iter[300/468], MSE loss:0.0698\n","Epoch [93/100] - Iter[400/468], MSE loss:0.0687\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [94/100] - Iter[100/468], MSE loss:0.0673\n","Epoch [94/100] - Iter[200/468], MSE loss:0.0712\n","Epoch [94/100] - Iter[300/468], MSE loss:0.0737\n","Epoch [94/100] - Iter[400/468], MSE loss:0.0715\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [95/100] - Iter[100/468], MSE loss:0.0731\n","Epoch [95/100] - Iter[200/468], MSE loss:0.0725\n","Epoch [95/100] - Iter[300/468], MSE loss:0.0695\n","Epoch [95/100] - Iter[400/468], MSE loss:0.0734\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [96/100] - Iter[100/468], MSE loss:0.0717\n","Epoch [96/100] - Iter[200/468], MSE loss:0.0712\n","Epoch [96/100] - Iter[300/468], MSE loss:0.0708\n","Epoch [96/100] - Iter[400/468], MSE loss:0.0736\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [97/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [97/100] - Iter[200/468], MSE loss:0.0730\n","Epoch [97/100] - Iter[300/468], MSE loss:0.0710\n","Epoch [97/100] - Iter[400/468], MSE loss:0.0697\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [98/100] - Iter[100/468], MSE loss:0.0669\n","Epoch [98/100] - Iter[200/468], MSE loss:0.0715\n","Epoch [98/100] - Iter[300/468], MSE loss:0.0743\n","Epoch [98/100] - Iter[400/468], MSE loss:0.0691\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n","Epoch [99/100] - Iter[100/468], MSE loss:0.0715\n","Epoch [99/100] - Iter[200/468], MSE loss:0.0773\n","Epoch [99/100] - Iter[300/468], MSE loss:0.0767\n","Epoch [99/100] - Iter[400/468], MSE loss:0.0689\n","\n","Average MSE Loss on Test set: 0.0694\n","Saved Best Model\n","\n","Epoch [100/100] - Iter[100/468], MSE loss:0.0733\n","Epoch [100/100] - Iter[200/468], MSE loss:0.0732\n","Epoch [100/100] - Iter[300/468], MSE loss:0.0683\n","Epoch [100/100] - Iter[400/468], MSE loss:0.0699\n","\n","Average MSE Loss on Test set: 0.0695\n","Saved Best Model\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XbMngj8kUAGs","colab_type":"text"},"source":["##Variational autoencoders"]},{"cell_type":"markdown","metadata":{"id":"8PwDHUcrUEP3","colab_type":"text"},"source":["#Covolutional autoencoders"]}]}